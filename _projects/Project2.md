---
title: "Parameter-Efficient Transformer"
header:
  image: /images/Roberta.jpeg
tech: "PyTorch, Hugging Face, LoRA, Python"
github: "https://github.com/tanaya09/CS-GY-6953-Deep-Learning-Project-2"
category: "AI/ML"
---

Fine-tuned RoBERTa-base on AG News using LoRA to train just 0.4% of parameters, achieving 92.3% accuracy in 3 epochs. Reduced GPU memory by 50% and enabled scalable deployment under 5 min/epoch.
